{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get user credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import getpass\n",
    "from pathlib import Path\n",
    "\n",
    "STORAGE_DIR = \"cv_storage\"\n",
    "EMAIL_REGEX = r\"^[\\w\\.-]+@[a-zA-Z\\d\\.-]+\\.[a-zA-Z]{2,}$\"\n",
    "\n",
    "\n",
    "def sanitize_email(email: str) -> str:\n",
    "    return re.sub(r\"[^\\w\\.-]\", \"_\", email.lower())\n",
    "\n",
    "\n",
    "def setup_environment():\n",
    "    \"\"\"\n",
    "    Prompts for:\n",
    "      - USER_EMAIL      (your email for CV storage & sending)\n",
    "      - GMAIL_APP_PASSWORD\n",
    "    Saves them in a user-specific credentials.json under cv_storage/{user_dir}\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        user_email = input(\"üìß Enter your email address (for CV storage & sending): \").strip().lower()\n",
    "        if re.match(EMAIL_REGEX, user_email):\n",
    "            break\n",
    "        print(\"‚ùå Invalid email format. Use name@domain.com\")\n",
    "\n",
    "    safe_email = sanitize_email(user_email)\n",
    "    user_dir = Path(STORAGE_DIR) / safe_email\n",
    "    user_dir.mkdir(parents=True, exist_ok=True)\n",
    "    credentials_path = user_dir / \"credentials.json\"\n",
    "\n",
    "    if credentials_path.exists():\n",
    "        update = input(\"‚ö†Ô∏è Credentials already exist. Do you want to update them? (y/N): \").strip().lower()\n",
    "        if update != \"y\":\n",
    "            print(\"‚úÖ Keeping existing credentials.\")\n",
    "            return\n",
    "\n",
    "    while True:\n",
    "        gmail_app_pwd = getpass.getpass(\"üîë Enter your Gmail App Password (for SMTP): \").strip()\n",
    "        if gmail_app_pwd:\n",
    "            break\n",
    "        print(\"‚ùå Password cannot be empty.\")\n",
    "\n",
    "    credentials = {\n",
    "        \"USER_EMAIL\": user_email,\n",
    "        \"GMAIL_APP_PASSWORD\": gmail_app_pwd\n",
    "    }\n",
    "\n",
    "    with open(credentials_path, \"w\") as f:\n",
    "        json.dump(credentials, f, indent=2)\n",
    "\n",
    "    print(f\"‚úÖ Credentials saved to {credentials_path}\")\n",
    "\n",
    "\n",
    "def get_user_email() -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Loads email from stored credentials.json\n",
    "    Returns (email, user_dir)\n",
    "    \"\"\"\n",
    "    for user_folder in Path(STORAGE_DIR).iterdir():\n",
    "        credentials_path = user_folder / \"credentials.json\"\n",
    "        if credentials_path.exists():\n",
    "            with open(credentials_path) as f:\n",
    "                credentials = json.load(f)\n",
    "                user_email = credentials.get(\"USER_EMAIL\")\n",
    "                if user_email:\n",
    "                    return user_email, str(user_folder)\n",
    "\n",
    "    raise RuntimeError(\"‚ùå No valid credentials found. Run setup_environment() first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llm config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if \"GROQ_API_KEY\" not in os.environ:\n",
    "    os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n",
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # grok_api_key='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'\n",
    "    # other params...\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# job extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "def extract_job_and_contact_info(job_paragraph: str) -> str:\n",
    "    prompt_extract = PromptTemplate.from_template(\"\"\"\n",
    "### JOB POSTING ANALYSIS TASK\n",
    "Analyze this job posting and extract structured information:\n",
    "\n",
    "{job_paragraph}\n",
    "\n",
    "### OUTPUT REQUIREMENTS:\n",
    "- Return JSON with two root keys: \"employer_info\" and \"position_details\"\n",
    "- Output ONLY valid JSON - no commentary\n",
    "- Include fields ONLY when explicitly mentioned\n",
    "- Maintain original wording for values\n",
    "- Handle all job types (medical, tech, education, etc.)\n",
    "\n",
    "1. EMPLOYER_INFO (hiring organization/individual):\n",
    "{{\n",
    "    \"full_name\": \"(if individual)\",\n",
    "    \"organization\": \"(company/institution name)\",\n",
    "    \"department\": \"(specific division/team)\",\n",
    "    \"industry\": \"(medical, tech, education, etc.)\",\n",
    "    \"contact\": {{\n",
    "        \"email\": \"(preferred)\",\n",
    "        \"phone\": \"(if provided)\",\n",
    "        \"website\": \"(career portal/LinkedIn)\"\n",
    "    }},\n",
    "    \"location\": {{\n",
    "        \"city\": \"(primary workplace)\",\n",
    "        \"country\": \"(if mentioned)\",\n",
    "        \"remote_options\": \"(hybrid/remote flags)\"\n",
    "    }},\n",
    "    \"organization_type\": \"(hospital, startup, university, etc.)\",\n",
    "    \"key_attributes\": [\"list\", \"of\", \"notable\", \"features\"]\n",
    "}}\n",
    "\n",
    "2. POSITION_DETAILS (job characteristics):\n",
    "{{\n",
    "    \"title\": \"(official job name)\",\n",
    "    \"type\": \"(full-time, contract, internship)\",\n",
    "    \"category\": \"(clinical, engineering, research, etc.)\",\n",
    "    \"level\": \"(junior, senior, principal)\",\n",
    "    \"salary\": {{\n",
    "        \"range\": \"(numbers or description)\",\n",
    "        \"currency\": \"(if specified)\",\n",
    "        \"bonuses\": \"(signing/performance bonuses)\"\n",
    "    }},\n",
    "    \"requirements\": {{\n",
    "        \"education\": \"(degrees/certifications)\",\n",
    "        \"experience\": \"(years/type)\",\n",
    "        \"skills\": [\"technical\", \"and\", \"soft\", \"skills\"],\n",
    "        \"licenses\": \"(industry-specific certifications)\"\n",
    "    }},\n",
    "    \"responsibilities\": [\"list\", \"of\", \"core\", \"duties\"],\n",
    "    \"benefits\": [\"healthcare\", \"retirement\", \"perks\"],\n",
    "    \"deadlines\": {{\n",
    "        \"application\": \"(DD/MM/YYYY or relative)\",\n",
    "        \"start_date\": \"(if specified)\"\n",
    "    }},\n",
    "    \"travel_requirements\": \"(percentage or description)\",\n",
    "    \"reporting_structure\": \"(supervisory relationships)\",\n",
    "    \"performance_metrics\": \"(KPIs/success measures)\"\n",
    "}}\n",
    "\n",
    "### OUTPUT ONLY VALID JSON WITH NO MARKDOWN FORMATTING\n",
    "\"\"\")\n",
    "    chain_extract = prompt_extract | llm\n",
    "    # prompt = prompt_extract.format(job_paragraph=job_paragraph)\n",
    "    response = chain_extract.invoke({'job_paragraph': job_paragraph})\n",
    "    return response.content\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "def validate_or_ask_email(extracted_json: dict) -> str:\n",
    "    # Check nested contact info first, then fallback to manual input\n",
    "    email = extracted_json.get(\"employer_info\", {}).get(\"contact\", {}).get(\"email\")\n",
    "    \n",
    "    if email and re.match(r\"^[\\w\\.-]+@[a-zA-Z\\d\\.-]+\\.[a-zA-Z]{2,}$\", email):\n",
    "        return email\n",
    "    \n",
    "    # If no valid email found, prompt user\n",
    "    print(\"\\n‚úâÔ∏è No valid email found in job posting. Please provide one:\")\n",
    "    while True:\n",
    "        manual_email = input(\"Contact email: \").strip()\n",
    "        if re.match(r\"^[\\w\\.-]+@[a-zA-Z\\d\\.-]+\\.[a-zA-Z]{2,}$\", manual_email):\n",
    "            return manual_email\n",
    "        print(\"Invalid format. Please enter a valid email address (e.g. name@company.com)\")\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "\n",
    "def run_outreach_intake():\n",
    "    global cv_data\n",
    "    print(\"üìã Paste the job posting below:\")\n",
    "    job_text = input(\"Job Posting: \")\n",
    "\n",
    "    print(\"\\nüîç Extracting details using LLM...\")\n",
    "    ext = extract_job_and_contact_info(job_text)\n",
    "    parser = JsonOutputParser()\n",
    "    extracted = parser.parse(ext)\n",
    "    print(\"\\n‚úÖ Extracted JSON:\\n\", extracted)\n",
    "\n",
    "    extracted_json = extracted\n",
    "    cv_data = extracted\n",
    "    \n",
    "    # Get nested contact information\n",
    "    contact_info = extracted_json.get(\"employer_info\", {}).get(\"contact\", {})\n",
    "    contact_email = validate_or_ask_email({\n",
    "        \"contact_email\": contact_info.get(\"email\"),\n",
    "        **extracted_json\n",
    "    })\n",
    "\n",
    "    print(\"\\nüìå Final Info:\")\n",
    "    print(f\"Industry: {extracted_json.get('employer_info', {}).get('industry', 'N/A')}\")\n",
    "    print(f\"Organization: {extracted_json.get('employer_info', {}).get('organization', 'N/A')}\")\n",
    "    print(\"Contact Email:\", contact_email)\n",
    "    print(\"\\nPosition Details:\")\n",
    "    for key, value in extracted_json.get(\"position_details\", {}).items():\n",
    "        print(f\"- {key.replace('_', ' ').title()}: {value or 'N/A'}\")\n",
    "\n",
    "    return {\n",
    "        \"employer_info\": extracted_json.get(\"employer_info\", {}),\n",
    "        \"position_details\": extracted_json.get(\"position_details\", {}),\n",
    "        \"contact_email\": contact_email,\n",
    "        \n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cv details extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from langchain_community.document_loaders import PyPDFLoader, UnstructuredWordDocumentLoader\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "from pydantic import BaseModel, Field\n",
    "import getpass\n",
    "\n",
    "# Enhanced configuration\n",
    "STORAGE_DIR = \"cv_storage\"\n",
    "EMAIL_REGEX = r\"^[\\w\\.-]+@[a-zA-Z\\d\\.-]+\\.[a-zA-Z]{2,}$\"\n",
    "# os.makedirs(STORAGE_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def upload_cv(user_dir: str) -> str:\n",
    "    \"\"\"Handle CV upload with comprehensive validation\"\"\"\n",
    "    while True:\n",
    "        file_path = input(\"üìÑ Enter CV path (.pdf/.docx): \").strip()\n",
    "        \n",
    "        if not os.path.isfile(file_path):\n",
    "            print(\"‚ùå File not found\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            if file_path.lower().endswith(\".pdf\"):\n",
    "                loader = PyPDFLoader(file_path)\n",
    "            elif file_path.lower().endswith(\".docx\"):\n",
    "                loader = UnstructuredWordDocumentLoader(file_path)\n",
    "            else:\n",
    "                print(\"‚ùå Unsupported format. Use PDF/DOCX\")\n",
    "                continue\n",
    "\n",
    "            documents = loader.load()\n",
    "            text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "            \n",
    "            # Save original file\n",
    "            safe_filename = re.sub(r\"[^\\w\\.-]\", \"_\", os.path.basename(file_path))\n",
    "            save_path = os.path.join(user_dir, safe_filename)\n",
    "            Path(save_path).write_bytes(Path(file_path).read_bytes())\n",
    "            \n",
    "            print(f\"‚úÖ CV saved: {save_path}\")\n",
    "            return text\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"üö® Error processing file: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "class ProfessionalProfile(BaseModel):\n",
    "    \"\"\"Universal professional profile model\"\"\"\n",
    "    full_name: str = Field(..., description=\"Full legal name\")\n",
    "    contact_email: str = Field(..., description=\"Primary contact email\")\n",
    "    phone: Optional[str] = Field(None, description=\"Contact phone number\")\n",
    "    summary: Optional[str] = Field(None, description=\"Professional summary\")\n",
    "    \n",
    "    education: List[Dict] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of educational achievements with degrees, institutions, and dates if provided\"\n",
    "    )\n",
    "    \n",
    "    experience: List[Dict] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Work history with job titles, companies, dates  if provided, and key achievements\"\n",
    "    )\n",
    "    \n",
    "    technical_skills: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Technical skills relevant to the industry\"\n",
    "    )\n",
    "    \n",
    "    certifications: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Professional certifications and licenses\"\n",
    "    )\n",
    "    \n",
    "    projects: List[Dict] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Notable projects with descriptions and outcomes\"\n",
    "    )\n",
    "    \n",
    "    industry_preferences: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Preferred industries or sectors\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "parser = JsonOutputParser(pydantic_object=ProfessionalProfile)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"**Professional Profile Analysis Task**\n",
    "Act as an expert career analyst with deep knowledge across industries (tech, healthcare, finance, engineering). \n",
    "Extract structured information while identifying transferable skills and cross-domain competencies.\n",
    "\"Please extract the following fields from the CV and return them in JSON format without any preamble: ...\"\n",
    "\n",
    "**Fields to Extract:**\n",
    "- full_name\n",
    "- contact_email\n",
    "- phone\n",
    "- summary\n",
    "- linkedin\n",
    "- github\n",
    "- education (list)\n",
    "- experience (list)\n",
    "- technical_skills (list)\n",
    "- soft_skills (list, optional)\n",
    "- certifications (list)\n",
    "- projects (list)\n",
    "- languages (optional)\n",
    "YOU CAN ADD AND SUBTRACT FIELDS ACCORDING TO PROVIDED CV AND INDUSTRY\n",
    "\n",
    "**Analysis Guidelines:**\n",
    "1. Core Identification:\n",
    "- Extract full legal name from header/contact section\n",
    "- Verify email format (name@domain.tld)\n",
    "- Identify phone numbers in international format (+XXX...)\n",
    "- Extract summary\n",
    "- linked in link(if provided)\n",
    "- github link(if provided)\n",
    "\n",
    "2. Education Analysis:\n",
    "- Parse degrees with majors/specializations\n",
    "- Flag accreditation status for institutions\n",
    "- Convert dates to MM/YYYY format  if provided\n",
    "- Highlight research projects/theses\n",
    "\n",
    "\n",
    "3. Experience Processing: \n",
    "- Separate employment history from internships\n",
    "- Identify technical/soft skill development\n",
    "- Quantify achievements (\"Increased X by Y%\")\n",
    "- Map technologies to industry standards\n",
    "\n",
    "4. Skill Extraction:\n",
    "- Categorize skills:\n",
    "  ‚Ä¢ Technical (tools/platforms)\n",
    "  ‚Ä¢ Methodologies (Agile, Six Sigma)\n",
    "  ‚Ä¢ Domain Knowledge (HIPAA, GAAP)\n",
    "- Identify skill maturity levels:\n",
    "  (Beginner < 1yr, Intermediate 1-3yr, Expert 3+yr)\n",
    "\n",
    "5. Cross-Industry Transfer Analysis:\n",
    "- Identify portable competencies between industries\n",
    "- Highlight leadership/management patterns\n",
    "- Extract crisis management evidence\n",
    "- Flag multilingual capabilities\n",
    "\n",
    "**Structured Output Requirements:**\n",
    "{format_instructions}\n",
    "\n",
    "**Content Processing Rules:**\n",
    "- Preserve original wording unless ambiguous\n",
    "- only if starting date is provided Convert relative dates (\"current\" ‚Üí {today})\n",
    "- Expand acronyms first occurrence (WHO ‚Üí World Health Organization)\n",
    "- Handle conflicting info (prioritize most recent)\n",
    "\n",
    "\n",
    "**Input Profile:**\n",
    "{text}\"\"\",\n",
    "    input_variables=[\"text\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": parser.get_format_instructions(),\n",
    "        \"today\": datetime.date.today().strftime(\"%m/%Y\")\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "def parse_cv(text: str) -> dict:\n",
    "    \"\"\"Process CV text through LLM parsing chain\"\"\"\n",
    "    try:\n",
    "        chain = prompt | llm | parser\n",
    "        result = chain.invoke({\"text\": text})\n",
    "        return dict(result)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error parsing CV: {str(e)}\")\n",
    "        return {}\n",
    "\n",
    "def save_parsed_data(data: dict, user_dir: str) -> None:\n",
    "    \"\"\"Save structured profile data\"\"\"\n",
    "    save_path = Path(user_dir) / \"profile_data.json\"\n",
    "    with open(save_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "    print(f\"üìÑ Profile data saved to {save_path}\")\n",
    "\n",
    "def display_profile_summary(user_dir: str) -> None:\n",
    "    \"\"\"Display formatted profile summary\"\"\"\n",
    "    data_file = Path(user_dir) / \"profile_data.json\"\n",
    "    if not data_file.exists():\n",
    "        print(\"‚ÑπÔ∏è No profile data available\")\n",
    "        return\n",
    "        \n",
    "    with open(data_file) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    print(\"\\nüåü Professional Summary:\")\n",
    "    print(f\"Name: {data.get('full_name', 'N/A')}\")\n",
    "    print(f\"Contact: {data.get('contact_email', 'N/A')} | {data.get('phone', 'N/A')}\")\n",
    "    print(f\"\\nüè´ Education ({len(data['education'])} entries)\")\n",
    "    print(f\"\\nüíº Experience ({len(data['experience'])} positions)\")\n",
    "    print(f\"\\nüõ†Ô∏è Technical Skills ({len(data['technical_skills'])} listed)\")\n",
    "\n",
    "def merge_with_llm(existing: dict, new: dict) -> dict:\n",
    "    \"\"\"Use LLM to intelligently merge two structured profile dicts.\"\"\"\n",
    "    if not existing:\n",
    "        return new\n",
    "    if not new:\n",
    "        return existing\n",
    "\n",
    "    try:\n",
    "        prompt_text = f\"\"\"\n",
    "You are a helpful assistant tasked with merging two structured professional profiles extracted from CVs. \n",
    "Your goal is to intelligently combine the data from both profiles, avoiding redundancy, preserving the most complete and informative entries, and resolving conflicts sensibly.\n",
    "\n",
    "Act as an expert career analyst with deep cross-industry knowledge (tech, healthcare, finance, engineering). \n",
    "You must identify transferable skills, merge overlapping entries, and preserve all unique information, especially for certifications.\n",
    "\n",
    "Please extract and return the following fields in raw JSON format **only**, without preamble or commentary.\n",
    "\n",
    "---\n",
    "**Fields to Extract:**\n",
    "- full_name\n",
    "- contact_email\n",
    "- phone\n",
    "- summary\n",
    "- linkedin\n",
    "- github\n",
    "- education (list)\n",
    "- experience (list)\n",
    "- technical_skills (list)\n",
    "- soft_skills (list, optional)\n",
    "- certifications (list)\n",
    "- projects (list)\n",
    "- languages (optional)\n",
    "YOU CAN ADD AND SUBTRACT FIELDS ACCORDING TO PROVIDED CV AND INDUSTRY\n",
    "---\n",
    "\n",
    "**Guidelines for Merging and Extraction:**\n",
    "\n",
    "1. **Core Info:**\n",
    "   - Extract full legal name from the header or contact block.\n",
    "   - Emails must be valid (e.g., name@domain.com).\n",
    "   - Phone numbers must be in international format (+XXX...).\n",
    "   - Include LinkedIn and GitHub links if found.\n",
    "\n",
    "2. **Education:**\n",
    "   - List all degrees and specializations.\n",
    "   - Include institution name, degree, field, start and end date (MM/YYYY).\n",
    "   - Highlight research projects or thesis titles if available.\n",
    "   - Avoid duplication; if same degree exists with more details, keep the more complete version.\n",
    "\n",
    "3. **Experience:**\n",
    "   - Distinguish jobs, internships, freelance, and volunteering.\n",
    "   - Include job title, company, duration, technologies used, and quantifiable outcomes.\n",
    "   - Keep the most recent or complete version of similar roles.\n",
    "   - Use consistent date format (MM/YYYY).\n",
    "\n",
    "4. **Skills:**\n",
    "   - Group skills into:\n",
    "     ‚Ä¢ Technical Skills (tools, platforms, libraries)\n",
    "     ‚Ä¢ Methodologies (Agile, Scrum, Six Sigma)\n",
    "     ‚Ä¢ Domain Knowledge (e.g., GDPR, HIPAA)\n",
    "   - Include experience levels if stated (e.g., Expert, Intermediate).\n",
    "\n",
    "5. **Certifications (‚úÖ Important):**\n",
    "   - Extract each certification with full name, issuing organization, and date (if available).\n",
    "   - Only merge certifications if the **exact full name and issuer match**.\n",
    "   - If titles are slightly different or have extra info (e.g., \"AWS Certified Developer ‚Äì Associate\" vs \"AWS Developer Cert\"), treat them as separate and preserve both.\n",
    "    \n",
    "6. **Projects:**\n",
    "   - Include title, description, technologies used, role (if specified), and duration.\n",
    "   - Projects may come from personal work, hackathons, university, or freelance.\n",
    "   - Merge only if titles and descriptions are identical or nearly identical.\n",
    "   - Preserve all distinct projects ‚Äî no limit.\n",
    "7. **Languages (if any):**\n",
    "   - Include spoken languages and proficiency if listed.\n",
    "\n",
    "8. **General Rules:**\n",
    "   - Avoid redundancy and merge smartly.\n",
    "   - Prioritize clarity, structure, and richness of information.\n",
    "   - Do not add placeholder or fabricated data.\n",
    "   - Output should be a valid JSON object.\n",
    "   \n",
    "\n",
    "---\n",
    "\n",
    "**Input Profiles:**\n",
    "\n",
    "Profile A (existing):\n",
    "{json.dumps(existing, indent=2)}\n",
    "\n",
    "Profile B (newly parsed):\n",
    "{json.dumps(new, indent=2)}\n",
    "\n",
    "Return ONLY the merged profile in raw JSON format.\n",
    "Do NOT include explanations or commentary. Just output the final merged JSON.\n",
    "\"\"\"\n",
    "    #     response = llm.invoke(prompt_text)\n",
    "    #     return json.loads(response.content.strip())\n",
    "    # except Exception as e:\n",
    "    #     print(f\"‚ö†Ô∏è LLM merge failed: {e}\")\n",
    "    #     return {**existing, **new}  # fallback: simple merge\n",
    "        response = llm.invoke(prompt_text)\n",
    "        raw_output = response.content.strip()\n",
    "\n",
    "        # üìå Safely extract the JSON part from the output\n",
    "        json_start = raw_output.find('{')\n",
    "        json_end = raw_output.rfind('}')\n",
    "        if json_start == -1 or json_end == -1:\n",
    "            raise ValueError(\"No JSON object found in LLM output\")\n",
    "\n",
    "        clean_json = raw_output[json_start:json_end+1]\n",
    "        return json.loads(clean_json)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è LLM merge failed: {e}\")\n",
    "        print(f\"‚ö†Ô∏è Raw LLM output:\\n{response.content if 'response' in locals() else 'No response'}\")\n",
    "        return {**existing, **new}  # fallback\n",
    "\n",
    "def run_cv_pipeline(user_dir: str) -> None:\n",
    "    \"\"\"Main execution flow\"\"\"\n",
    "    if not any(Path(user_dir).iterdir()):\n",
    "        print(\"üì• Initial profile setup\")\n",
    "        cv_text = upload_cv(user_dir)\n",
    "        parsed_data = parse_cv(cv_text)\n",
    "        save_parsed_data(parsed_data, user_dir)\n",
    "    else:\n",
    "        manage_cv(user_dir)\n",
    "        \n",
    "\n",
    "def manage_cv(user_dir: str) -> None:\n",
    "    \"\"\"Enhanced CV management system\"\"\"\n",
    "    while True:\n",
    "        action = input(\"\\nChoose: [U]pload new, [D]elete, [V]iew, [E]xit: \").strip().lower()\n",
    "        \n",
    "        # if action == \"u\":\n",
    "        #     text = upload_cv(user_dir)\n",
    "        #     parsed = parse_cv(text)\n",
    "        #     save_parsed_data(parsed, user_dir)\n",
    "        if action == \"u\":\n",
    "            text = upload_cv(user_dir)\n",
    "            new_parsed = parse_cv(text)\n",
    "\n",
    "            # Load existing data if it exists\n",
    "            existing_file = Path(user_dir) / \"profile_data.json\"\n",
    "            if existing_file.exists():\n",
    "                with open(existing_file) as f:\n",
    "                    existing_parsed = json.load(f)\n",
    "                merged_data = merge_with_llm(existing_parsed, new_parsed)\n",
    "                print(\"üîÑ Merged new CV with existing profile using LLM.\")\n",
    "            else:\n",
    "                merged_data = new_parsed\n",
    "                print(\"üÜï No existing profile found. Saving new data.\")\n",
    "\n",
    "            save_parsed_data(merged_data, user_dir)\n",
    "\n",
    "        elif action == \"d\":\n",
    "            confirm = input(\"‚ö†Ô∏è Delete ALL profile data? (y/n): \").lower()\n",
    "            if confirm == \"y\":\n",
    "                for item in Path(user_dir).glob(\"*\"):\n",
    "                    item.unlink()\n",
    "                print(\"üóëÔ∏è Profile data deleted\")\n",
    "        elif action == \"v\":\n",
    "            display_profile_summary(user_dir)\n",
    "        elif action == \"e\":\n",
    "            break\n",
    "        else:\n",
    "            print(\"‚ùå Invalid option\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## custom cv maker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "from typing import Dict, Literal\n",
    "from pathlib import Path\n",
    "import json\n",
    "from fpdf import FPDF\n",
    "import re\n",
    "\n",
    "def clean_unicode(text: str) -> str:\n",
    "    replacements = {\n",
    "        '\\u2013': '-',  # en dash\n",
    "        '\\u2014': '-',  # em dash\n",
    "        '\\u2018': \"'\",  # left single quote\n",
    "        '\\u2019': \"'\",  # right single quote\n",
    "        '\\u201c': '\"',  # left double quote\n",
    "        '\\u201d': '\"',  # right double quote\n",
    "        '\\u2026': '...',  # ellipsis\n",
    "    }\n",
    "    for bad, good in replacements.items():\n",
    "        text = text.replace(bad, good)\n",
    "    return text\n",
    "\n",
    "def generate_custom_cv(\n",
    "    user_dir: str,\n",
    "    job_posting_info: Dict,\n",
    "    llm,\n",
    "    action: Literal[\"preview\", \"save\", \"edit\"] = \"preview\",\n",
    "    edit_instructions: str = \"\"\n",
    ") -> str:\n",
    "    # Load user profile\n",
    "    existing_file = Path(user_dir) / \"profile_data.json\"\n",
    "    if not existing_file.exists():\n",
    "        raise FileNotFoundError(\"User profile_data.json not found.\")\n",
    "\n",
    "    with open(existing_file, \"r\") as f:\n",
    "        user_data = json.load(f)\n",
    "\n",
    "    generated_cv_path = Path(user_dir) / \"latest_cv.txt\"\n",
    "    print(user_data)\n",
    "    print(cv_data)\n",
    "    # Main CV generation\n",
    "    if action == \"preview\":\n",
    "        # Initial prompt\n",
    "        prompt = PromptTemplate(\n",
    "        input_variables=[\"user_data\", \"job_posting_info\"],\n",
    "        template=\"\"\"\n",
    "You are an expert resume writer specializing in ATS-optimized, role-targeted CVs.\n",
    "\n",
    "Generate a clean, plain text CV using this structure:\n",
    "\n",
    "[Full Name]\n",
    "[Email] | [Phone] | [LinkedIn/GitHub if available]\n",
    "\n",
    "SUMMARY:\n",
    "- 1-2 sentence professional summary with 3 keywords from job description\n",
    "\n",
    "EXPERIENCE:\n",
    "[Job Title] - [Company], [Location]\n",
    "[MM/YYYY - MM/YYYY]\n",
    "- CAR-formatted bullet points (Challenge-Action-Result)\n",
    "- Include relevant technical keywords\n",
    "- Quantify achievements where possible\n",
    "\n",
    "EDUCATION:\n",
    "[Degree] - [Institution]\n",
    "[MM/YYYY - MM/YYYY]\n",
    "\n",
    "SKILLS:\n",
    "- Technical skills matching job requirements\n",
    "- Tools/platforms from job description\n",
    "\n",
    "PROJECTS (if relevant to position):\n",
    "[Project Name]\n",
    "- Brief description showing job-relevant skills\n",
    "\n",
    "---\n",
    "\n",
    "Candidate Info:\n",
    "{user_data}\n",
    "\n",
    "Job Posting Details:\n",
    "{job_posting_info}\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    ")\n",
    "        chain: Runnable = prompt | llm\n",
    "        try:\n",
    "            result = chain.invoke({\n",
    "                \"user_data\": json.dumps(user_data, indent=2),\n",
    "                \"job_posting_info\": json.dumps(job_posting_info, indent=2),\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Error during LLM invocation:\", e)\n",
    "            return \"Failed to generate CV. Check the logs.\"\n",
    "\n",
    "        if isinstance(result, str):\n",
    "            cv_text = clean_unicode(result.strip())\n",
    "        elif isinstance(result, dict) and 'content' in result:\n",
    "            cv_text = clean_unicode(result['content'].strip())\n",
    "        elif hasattr(result, 'content'):\n",
    "            cv_text = clean_unicode(result.content.strip())\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected LLM result format.\")\n",
    "\n",
    "        generated_cv_path.write_text(cv_text, encoding=\"utf-8\")\n",
    "        return cv_text\n",
    "\n",
    "    elif action == \"edit\":\n",
    "        if not generated_cv_path.exists():\n",
    "            raise FileNotFoundError(\"Preview the CV first before editing.\")\n",
    "        \n",
    "        previous_cv = generated_cv_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "        edit_prompt = PromptTemplate(\n",
    "            input_variables=['previous_cv','edit_instructions',   'user_data', 'job_posting_info'],\n",
    "            template=\"\"\"\n",
    "You are a professional resume editor.\n",
    "\n",
    "Here is the current CV:\n",
    "---\n",
    "{previous_cv}\n",
    "---\n",
    "\n",
    "Here are the instructions from the user:\n",
    "---\n",
    "{edit_instructions}\n",
    "---\n",
    "\n",
    "\n",
    "üìÑ Candidate Info:\n",
    "---\n",
    "{user_data}\n",
    "---\n",
    "üßæ Job Posting:\n",
    "---\n",
    "{job_posting_info}\n",
    "---\n",
    "\n",
    "Revise the CV accordingly while preserving the ATS-optimized formatting.\n",
    "\"\"\"\n",
    "        )\n",
    "        chain: Runnable = edit_prompt | llm\n",
    "        result = chain.invoke({\n",
    "            \"previous_cv\": previous_cv,\n",
    "            \"edit_instructions\": edit_instructions,\n",
    "            \"user_data\": json.dumps(user_data, indent=2),\n",
    "            \"job_posting_info\": json.dumps(job_posting_info, indent=2),\n",
    "            \n",
    "        })\n",
    "        cv_text = clean_unicode(result.content.strip())\n",
    "        generated_cv_path.write_text(cv_text, encoding=\"utf-8\")\n",
    "        return cv_text\n",
    "\n",
    "    elif action == \"save\":\n",
    "        if not generated_cv_path.exists():\n",
    "            raise FileNotFoundError(\"Preview the CV first before saving.\")\n",
    "        cv_text = generated_cv_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "        output_path = Path(user_dir) / \"generated_cv.pdf\"\n",
    "        pdf = FPDF()\n",
    "        pdf.set_auto_page_break(auto=True, margin=15)\n",
    "        pdf.add_page()\n",
    "        pdf.set_font(\"Arial\", size=11)\n",
    "        for line in cv_text.split(\"\\n\"):\n",
    "            pdf.multi_cell(0, 10, txt=line.strip())\n",
    "\n",
    "        pdf.output(str(output_path))\n",
    "        return f\"PDF saved at: {output_path.resolve()}\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid action. Use 'preview', 'edit', or 'save'.\")\n",
    "\n",
    "\n",
    "def run_cv_interactive_flow(user_dir: str, job_posting_info: Dict, llm):\n",
    "    print(\"Generating preview...\")\n",
    "    preview = generate_custom_cv(user_dir, job_posting_info, llm, action=\"preview\")\n",
    "    print(\"\\nüìÑ Preview of Generated CV:\\n\")\n",
    "    print(preview)\n",
    "\n",
    "    while True:\n",
    "        user_action = input(\"\\nWhat would you like to do next? [save/edit/exit]: \").strip().lower()\n",
    "\n",
    "        if user_action == \"save\":\n",
    "            result = generate_custom_cv(user_dir, job_posting_info, llm, action=\"save\")\n",
    "            print(result)\n",
    "            break\n",
    "\n",
    "        elif user_action == \"edit\":\n",
    "            instructions = input(\"Enter your editing instructions for the CV:\\n\")\n",
    "            edited = generate_custom_cv(user_dir, job_posting_info, llm, action=\"edit\", edit_instructions=instructions)\n",
    "            print(\"\\nüìù Updated CV:\\n\")\n",
    "            print(edited)\n",
    "\n",
    "        elif user_action == \"exit\":\n",
    "            print(\"Exiting without saving.\")\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            print(\"‚ùå Invalid input. Please choose: save / edit / exit\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cover letter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cover_letter(\n",
    "    user_dir: str,\n",
    "    job_posting_info: Dict,\n",
    "    llm,\n",
    "    action: Literal[\"preview\", \"edit\"] = \"preview\",\n",
    "    edit_instructions: str = \"\"\n",
    ") -> str:\n",
    "    profile_path = Path(user_dir) / \"profile_data.json\"\n",
    "    if not profile_path.exists():\n",
    "        raise FileNotFoundError(\"User profile_data.json not found.\")\n",
    "\n",
    "    with open(profile_path, \"r\") as f:\n",
    "        user_data = json.load(f)\n",
    "\n",
    "    generated_letter_path = Path(user_dir) / \"latest_cover_letter.txt\"\n",
    "\n",
    "    if action == \"preview\":\n",
    "        prompt = PromptTemplate(\n",
    "            input_variables=[\"user_data\", \"job_posting_info\"],\n",
    "            template=\"\"\"\n",
    "You are an expert in writing tailored, concise, and professional cover letters for job applications.\n",
    "\n",
    "Use the following data to generate a compelling, personalized cover letter.\n",
    "\n",
    "---\n",
    "\n",
    "üë§ **Candidate Profile**\n",
    "{user_data}\n",
    "\n",
    "üßæ **Job Posting**\n",
    "{job_posting_info}\n",
    "\n",
    "---\n",
    "\n",
    "‚úçÔ∏è **Instructions**:\n",
    "- Use standard business letter format: Address, Greeting, 3‚Äì4 short paragraphs, and Signature.\n",
    "- Keep it under 350 words.\n",
    "- Address the hiring manager if possible (\"Dear Hiring Manager\" if not known).\n",
    "- Mention 2‚Äì3 core skills or experiences that match the job.\n",
    "- End with a polite call to action.\n",
    "- Use clear, professional language.\n",
    "\n",
    "Now generate the cover letter.\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "        chain: Runnable = prompt | llm\n",
    "        result = chain.invoke({\n",
    "            \"user_data\": json.dumps(user_data, indent=2),\n",
    "            \"job_posting_info\": json.dumps(job_posting_info, indent=2),\n",
    "        })\n",
    "\n",
    "        letter_text = clean_unicode(result.content.strip())\n",
    "        generated_letter_path.write_text(letter_text, encoding=\"utf-8\")\n",
    "        return letter_text\n",
    "\n",
    "    elif action == \"edit\":\n",
    "        if not generated_letter_path.exists():\n",
    "            raise FileNotFoundError(\"Preview the letter first before editing.\")\n",
    "\n",
    "        previous_letter = generated_letter_path.read_text(encoding=\"utf-8\")\n",
    "\n",
    "        edit_prompt = PromptTemplate(\n",
    "            input_variables=[\"previous_letter\", \"edit_instructions\"],\n",
    "            template=\"\"\"\n",
    "You are a professional writing assistant.\n",
    "\n",
    "Here's the current cover letter:\n",
    "---\n",
    "{previous_letter}\n",
    "---\n",
    "\n",
    "Apply the following editing instructions:\n",
    "---\n",
    "{edit_instructions}\n",
    "---\n",
    "\n",
    "Return the updated cover letter.\n",
    "\"\"\"\n",
    "        )\n",
    "        chain: Runnable = edit_prompt | llm\n",
    "        result = chain.invoke({\n",
    "            \"previous_letter\": previous_letter,\n",
    "            \"edit_instructions\": edit_instructions,\n",
    "        })\n",
    "\n",
    "        letter_text = clean_unicode(result.content.strip())\n",
    "        generated_letter_path.write_text(letter_text, encoding=\"utf-8\")\n",
    "        return letter_text\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid action. Use 'preview' or 'edit'.\")\n",
    "\n",
    "\n",
    "def run_cover_letter_interactive_flow(user_dir: str, job_posting_info: Dict, llm):\n",
    "    print(\"\\n‚úâÔ∏è Generating Cover Letter Preview...\\n\")\n",
    "    preview = generate_cover_letter(user_dir, job_posting_info, llm, action=\"preview\")\n",
    "    print(preview)\n",
    "\n",
    "    while True:\n",
    "        user_action = input(\"\\nWhat would you like to do with the cover letter? [edit/exit]: \").strip().lower()\n",
    "\n",
    "        if user_action == \"edit\":\n",
    "            instructions = input(\"Enter your editing instructions for the cover letter:\\n\")\n",
    "            updated = generate_cover_letter(user_dir, job_posting_info, llm, action=\"edit\", edit_instructions=instructions)\n",
    "            print(\"\\nüìù Updated Cover Letter:\\n\")\n",
    "            print(updated)\n",
    "\n",
    "        elif user_action == \"exit\":\n",
    "            print(\"Finished with cover letter.\")\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            print(\"‚ùå Invalid input. Choose 'edit' or 'exit'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## email sending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_email_with_cv(user_dir: str, recipient_email: str):\n",
    "    from pathlib import Path\n",
    "    import json\n",
    "    import os\n",
    "    from email.message import EmailMessage\n",
    "    import smtplib, ssl\n",
    "\n",
    "    user_dir = Path(user_dir)\n",
    "\n",
    "    # Load credentials\n",
    "    with open(user_dir / \"credentials.json\") as f:\n",
    "        creds = json.load(f)\n",
    "\n",
    "    sender_email = creds[\"USER_EMAIL\"]\n",
    "    app_password = creds[\"GMAIL_APP_PASSWORD\"]\n",
    "\n",
    "    # Get cover letter\n",
    "    cover_letter_path = user_dir / \"latest_cover_letter.txt\"\n",
    "    if not cover_letter_path.exists():\n",
    "        raise FileNotFoundError(f\"‚ùå Cover letter not found at {cover_letter_path}\")\n",
    "    cover_letter_text = cover_letter_path.read_text()\n",
    "\n",
    "    # Get generated CV\n",
    "    pdf_path = user_dir / \"generated_cv.pdf\"\n",
    "    if not pdf_path.exists():\n",
    "        raise FileNotFoundError(f\"‚ùå Generated CV not found at {pdf_path}\")\n",
    "\n",
    "    # Compose email\n",
    "    msg = EmailMessage()\n",
    "    msg[\"Subject\"] = f\"Regarding Job Application ‚Äì {sender_email.split('@')[0].title()}\"\n",
    "    msg[\"From\"] = sender_email\n",
    "    msg[\"To\"] = recipient_email\n",
    "    msg.set_content(cover_letter_text)\n",
    "\n",
    "    # Attach CV\n",
    "    pdf_data = pdf_path.read_bytes()\n",
    "    msg.add_attachment(pdf_data, maintype='application', subtype='pdf', filename=pdf_path.name)\n",
    "\n",
    "    # Send via Gmail SMTP\n",
    "    context = ssl.create_default_context()\n",
    "    print(\"üì° Connecting to Gmail SMTP...\")\n",
    "    with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context) as smtp:\n",
    "        print(\"üîê Logging in...\")\n",
    "        smtp.login(sender_email, app_password)\n",
    "        print(\"üì§ Sending email...\")\n",
    "        smtp.send_message(msg)\n",
    "        print(\"‚úÖ Email sent\")\n",
    "\n",
    "   \n",
    "\n",
    "    print(f\"‚úÖ Email sent to {recipient_email} with CV: {pdf_path.name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Keeping existing credentials.\n",
      "üìÇ Your directory: cv_storage\\shahmeergull20_gmail.com\n",
      "üìß Your email: shahmeergull20@gmail.com\n",
      "üìã Paste the job posting below:\n",
      "\n",
      "üîç Extracting details using LLM...\n",
      "\n",
      "‚úÖ Extracted JSON:\n",
      " {'employer_info': {'organization': 'Es Magico AI Studio', 'industry': 'tech', 'contact': {'email': 'karan.trehan@esmagico.in'}, 'location': {'city': 'Mumbai', 'remote_options': 'hybrid'}, 'organization_type': 'startup', 'key_attributes': []}, 'position_details': {'title': 'Intern', 'type': 'full-time & paid internship', 'category': 'AI', 'level': '', 'salary': {'range': '', 'currency': '', 'bonuses': ''}, 'requirements': {'education': 'BTech / BCA / MTech / MCA / BSc / etc.', 'experience': '', 'skills': ['Python scripting', 'AI (RAGs, Agents, Prompting)'], 'licenses': ''}, 'responsibilities': [], 'benefits': [], 'deadlines': {'application': '', 'start_date': ''}, 'travel_requirements': 'thrice a week', 'reporting_structure': '', 'performance_metrics': ''}}\n",
      "\n",
      "üìå Final Info:\n",
      "Industry: tech\n",
      "Organization: Es Magico AI Studio\n",
      "Contact Email: karan.trehan@esmagico.in\n",
      "\n",
      "Position Details:\n",
      "- Title: Intern\n",
      "- Type: full-time & paid internship\n",
      "- Category: AI\n",
      "- Level: N/A\n",
      "- Salary: {'range': '', 'currency': '', 'bonuses': ''}\n",
      "- Requirements: {'education': 'BTech / BCA / MTech / MCA / BSc / etc.', 'experience': '', 'skills': ['Python scripting', 'AI (RAGs, Agents, Prompting)'], 'licenses': ''}\n",
      "- Responsibilities: N/A\n",
      "- Benefits: N/A\n",
      "- Deadlines: {'application': '', 'start_date': ''}\n",
      "- Travel Requirements: thrice a week\n",
      "- Reporting Structure: N/A\n",
      "- Performance Metrics: N/A\n",
      "Generating preview...\n",
      "{'full_name': 'Shahmeer Gull', 'contact_email': 'shahmeergull20@gmail.com', 'phone': '+92-309-0654885', 'summary': 'Machine Learning Enthusiast specializing in fine-tuning large language models (LLMs), Retrieval-Augmented Generation (RAG), and LangChain to build scalable AI solutions.', 'linkedin': None, 'github': None, 'education': [{'degree': \"Bachelor's in Computer Science\", 'institution': 'COMSATS University', 'date': '2020-2024', 'certifications': ['AI for Public Health', 'Fundamentals of AI Agents Using RAG and LangChain', 'Generative AI Advance Fine-Tuning for LLMs', 'Generative AI and LLMs: Architecture and Data Preparation', 'AI for Good']}], 'experience': [{'company': 'ByteSight Pvt Ltd', 'job_title': 'MERN Stack Intern', 'location': 'islamabad', 'date': '2022-2023', 'achievements': ['Developed and styled responsive front-end interfaces using React.js and Tailwind CSS.', 'Integrated APIs and enhanced user experience with dynamic, interactive components.', 'Collaborated with team to maintain code quality and implement UI improvements.'], 'technologies': ['React.js', 'Tailwind CSS', 'Node.js', 'Express.js', 'RESTful APIs', 'Git', 'GitHub', 'Vercel']}, {'company': 'RASTA TECH', 'job_title': 'AI Engineer intern', 'location': 'remote', 'date': '2022-2023', 'achievements': ['Improved ML/DL models to enhance chatbot responses and user experience.', 'Preprocessed conversational data, extracted relevant features (like intent, sentiment), and evaluated model performance.', 'Built and fine-tuned models using PyTorch and scikit-learn for tasks like intent classification and response generation', 'Supported development and optimization of ML/DL models for business applications.', 'Handled data preprocessing, feature engineering, and model evaluation.', 'Collaborated on AI solutions using tools like PyTorch and scikit-learn'], 'technologies': ['PyTorch', 'scikit-learn', 'ML/DL models', 'React.js', 'Tailwind CSS', 'Node.js', 'Express.js', 'RESTful APIs', 'Git', 'GitHub', 'Vercel']}], 'technical_skills': ['LLMs & Fine-Tuning: RAG, LangChain, Chroma DB, Prompt Engineering, RLHF, Model Optimization', 'Deep Learning: Transformer Models, Attention Mechanisms, Sequence Modeling', 'AI Libraries: PyTorch, TensorFlow, Hugging Face, Scikit-Learn, Keras', 'Full-Stack Development (MERN): Frontend: React.js, Tailwind CSS, Responsive Design, Backend: Node.js, Express.js, RESTful APIs, Tools: Git, GitHub, Vercel', 'Programming Languages: Python, JavaScript, SQL, C++', 'Data Analysis & Engineering: Data Handling: Pandas, NumPy, Feature Engineering, Efficient Preprocessing, Databases: SQL, MongoDB, Firebase'], 'soft_skills': [], 'certifications': ['AI for Public Health', 'Fundamentals of AI Agents Using RAG and LangChain', 'Generative AI Advance Fine-Tuning for LLMs', 'Generative AI and LLMs: Architecture and Data Preparation', 'AI for Good'], 'projects': [{'name': 'MarkVista ‚Äì AI-Powered Crypto Prediction & Risk Management Platform', 'description': 'Developed ML prediction models for crypto price prediction with real-time market data, Engineered user-configurable risk profiles to manage portfolio risk dynamically, Integrated broker APIs for automated trading execution with responsive risk controls, Visualized analytics through a dynamic portfolio dashboard, Built secure infrastructure and simulated community Q&A, Streamlined user workflows into a unified interface', 'technologies': ['FastAPI', 'Python', 'ML Models', 'React JS', 'MySQL', 'Figma', 'CoinGecko API', 'Postman']}, {'name': 'Private Document Summarization with RAG, LangChain, and LLMs', 'description': 'Built a secure RAG pipeline using LangChain, Chroma DB, and Hugging Face embeddings to summarize private documents locally, Integrated IBM Watsonx.ai LLMs (FLAN-UL2, Llama-3-70B) for summarization and Q&A with prompt tuning and memory, Created a context-aware chatbot using RetrievalQA and ConversationalBufferMemory, Automated internal document analysis, reducing manual review time while maintaining privacy compliance', 'technologies': ['IBM Watsonx.ai', 'LangChain', 'Hugging Face Embeddings', 'Chroma DB', 'LLMs', 'Python']}, {'name': 'RAG-based Content Evaluation for Children‚Äôs Media', 'description': \"Embedding Generation: Used BERT from Hugging Face's Transformers library to generate embeddings for song lyrics and predefined questions, Similarity Measurement: Implemented dot product and cosine similarity to measure the relevance between song embeddings and question embeddings, Visualization: Applied t-SNE for visualizing high-dimensional embeddings in a 3D space to analyze clustering and patterns, RAG Workflow: Combined a retriever (to fetch relevant embeddings) and a generator (to provide responses) for efficient content evaluation\", 'technologies': ['PyTorch', 'BERT', 'Hugging Face', 'Scikit-learn', 'Matplotlib']}, {'name': 'Air Quality Analysis & Modeling ‚Äì Bogot√° Pollution Data', 'description': 'Cleaned multiyear pollution, weather & location data from public sources, Analyzed trends using correlation, time series, and geospatial visualizations, Imputed missing values using linear interpolation, KNN, and neural networks; evaluated with MAE/RMSE, Developed a hybrid model combining KNN and neural networks for robust pollution estimation, Trained and validated models using cross-validation at under-monitored sites, Visualized results with GeoPandas, Folium, and a Jupyter dashboard', 'technologies': ['Python', 'Pandas', 'NumPy', 'Scikit-learn', 'Keras', 'Matplotlib', 'Seaborn', 'GeoPandas']}, {'name': 'Cold Email Generator for Business Outreach', 'description': 'Built an end-to-end tool to generate personalized B2B emails by analyzing job postings, Scraped career pages and extracted job info using LangChain + Groq API (Llama-3.1-8B), Parsed data into JSON, matched with portfolio links via Chroma DB for contextual retrieval, Tuned prompts to simulate a business development tone', 'technologies': ['Groq API', 'LangChain', 'Streamlit', 'Chroma DB', 'Python', 'Llama-3.1-8B', 'Web Scraping', 'JSON Parsing']}], 'languages': ['English', 'Urdu']}\n",
      "{'employer_info': {'organization': 'Es Magico AI Studio', 'industry': 'tech', 'contact': {'email': 'karan.trehan@esmagico.in'}, 'location': {'city': 'Mumbai', 'remote_options': 'hybrid'}, 'organization_type': 'startup', 'key_attributes': []}, 'position_details': {'title': 'Intern', 'type': 'full-time & paid internship', 'category': 'AI', 'level': '', 'salary': {'range': '', 'currency': '', 'bonuses': ''}, 'requirements': {'education': 'BTech / BCA / MTech / MCA / BSc / etc.', 'experience': '', 'skills': ['Python scripting', 'AI (RAGs, Agents, Prompting)'], 'licenses': ''}, 'responsibilities': [], 'benefits': [], 'deadlines': {'application': '', 'start_date': ''}, 'travel_requirements': 'thrice a week', 'reporting_structure': '', 'performance_metrics': ''}}\n",
      "\n",
      "üìÑ Preview of Generated CV:\n",
      "\n",
      "Here's a clean, plain text CV optimized for ATS and tailored to the provided job posting:\n",
      "\n",
      "Shahmeer Gull\n",
      "shahmeergull20@gmail.com | +92-309-0654885 | (LinkedIn/GitHub: Not Available)\n",
      "\n",
      "**SUMMARY:**\n",
      "AI Enthusiast with expertise in fine-tuning large language models (LLMs), Retrieval-Augmented Generation (RAG), and LangChain to build scalable AI solutions. Proficient in Python scripting, AI (RAGs, Agents, Prompting), and full-stack development.\n",
      "\n",
      "**EXPERIENCE:**\n",
      "\n",
      "**AI Engineer Intern - RASTA TECH (Remote)**\n",
      "2022-2023\n",
      "\n",
      "- **Challenge:** Improved ML/DL models to enhance chatbot responses and user experience.\n",
      "- **Action:** Preprocessed conversational data, extracted relevant features (like intent, sentiment), and evaluated model performance using PyTorch and scikit-learn.\n",
      "- **Result:** Built and fine-tuned models for tasks like intent classification and response generation, resulting in improved chatbot accuracy and user engagement.\n",
      "\n",
      "**MERN Stack Intern - ByteSight Pvt Ltd (Islamabad)**\n",
      "2022-2023\n",
      "\n",
      "- **Challenge:** Developed and styled responsive front-end interfaces using React.js and Tailwind CSS.\n",
      "- **Action:** Integrated APIs and enhanced user experience with dynamic, interactive components.\n",
      "- **Result:** Collaborated with team to maintain code quality and implement UI improvements, resulting in a 30% increase in user engagement.\n",
      "\n",
      "**EDUCATION:**\n",
      "\n",
      "Bachelor's in Computer Science - COMSATS University (2020-2024)\n",
      "\n",
      "**SKILLS:**\n",
      "\n",
      "- **Technical Skills:** Python scripting, AI (RAGs, Agents, Prompting), PyTorch, scikit-learn, React.js, Tailwind CSS, Node.js, Express.js, RESTful APIs, Git, GitHub, Vercel\n",
      "- **Tools/Platforms:** LangChain, Chroma DB, Hugging Face Embeddings, IBM Watsonx.ai, FastAPI, MySQL, Figma, CoinGecko API, Postman\n",
      "\n",
      "**PROJECTS:**\n",
      "\n",
      "**MarkVista - AI-Powered Crypto Prediction & Risk Management Platform**\n",
      "\n",
      "- Developed ML prediction models for crypto price prediction with real-time market data.\n",
      "- Engineered user-configurable risk profiles to manage portfolio risk dynamically.\n",
      "- Integrated broker APIs for automated trading execution with responsive risk controls.\n",
      "\n",
      "**Private Document Summarization with RAG, LangChain, and LLMs**\n",
      "\n",
      "- Built a secure RAG pipeline using LangChain, Chroma DB, and Hugging Face embeddings to summarize private documents locally.\n",
      "- Integrated IBM Watsonx.ai LLMs (FLAN-UL2, Llama-3-70B) for summarization and Q&A with prompt tuning and memory.\n",
      "\n",
      "**RAG-based Content Evaluation for Children's Media**\n",
      "\n",
      "- Embedded Generation: Used BERT from Hugging Face's Transformers library to generate embeddings for song lyrics and predefined questions.\n",
      "- Similarity Measurement: Implemented dot product and cosine similarity to measure the relevance between song embeddings and question embeddings.\n",
      "\n",
      "**Air Quality Analysis & Modeling - Bogot√° Pollution Data**\n",
      "\n",
      "- Cleaned multiyear pollution, weather & location data from public sources.\n",
      "- Analyzed trends using correlation, time series, and geospatial visualizations.\n",
      "- Developed a hybrid model combining KNN and neural networks for robust pollution estimation.\n",
      "\n",
      "**Cold Email Generator for Business Outreach**\n",
      "\n",
      "- Built an end-to-end tool to generate personalized B2B emails by analyzing job postings.\n",
      "- Scraped career pages and extracted job info using LangChain + Groq API (Llama-3.1-8B).\n",
      "- Parsed data into JSON, matched with portfolio links via Chroma DB for contextual retrieval.\n",
      "Exiting without saving.\n",
      "\n",
      "‚úâÔ∏è Generating Cover Letter Preview...\n",
      "\n",
      "[Your Address]\n",
      "[City, State, Zip]\n",
      "[Email Address]\n",
      "[Phone Number]\n",
      "[Date]\n",
      "\n",
      "Karan Trehan\n",
      "Es Magico AI Studio\n",
      "Mumbai\n",
      "\n",
      "Dear Hiring Manager,\n",
      "\n",
      "I am writing to express my interest in the Intern position at Es Magico AI Studio, as advertised on [Job Posting Platform]. As a Machine Learning Enthusiast with a strong background in fine-tuning large language models (LLMs), Retrieval-Augmented Generation (RAG), and LangChain, I am confident that I can contribute to the success of your team.\n",
      "\n",
      "With a Bachelor's degree in Computer Science from COMSATS University, I have developed a solid foundation in AI and ML concepts. My experience as an AI Engineer intern at RASTA TECH has equipped me with expertise in building and fine-tuning ML/DL models using PyTorch and scikit-learn. I have also worked on various projects, including MarkVista - AI-Powered Crypto Prediction & Risk Management Platform, where I developed ML prediction models for crypto price prediction and integrated broker APIs for automated trading execution. My proficiency in Python, JavaScript, and SQL, along with my experience in full-stack development using MERN, makes me an ideal candidate for this role.\n",
      "\n",
      "I am particularly drawn to this internship because of the opportunity to work on AI-related projects and contribute to the growth of Es Magico AI Studio. My skills in RAG, LangChain, and LLMs align with the job requirements, and I am excited about the prospect of working with a team that shares my passion for AI. I would welcome the opportunity to discuss my application and how I can contribute to the success of your team.\n",
      "\n",
      "Thank you for considering my application. I look forward to the opportunity to discuss this further. Please feel free to contact me at [Your Email Address] or [Your Phone Number].\n",
      "\n",
      "Sincerely,\n",
      "\n",
      "Shahmeer Gull\n",
      "Finished with cover letter.\n",
      "üì° Connecting to Gmail SMTP...\n",
      "üîê Logging in...\n",
      "üì§ Sending email...\n",
      "‚úÖ Email sent\n",
      "‚úÖ Email sent to shahmeergull20@gmail.com with CV: generated_cv.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Prompt once (or load existing) for all creds including GROQ_API_KEY\n",
    "    setup_environment()\n",
    "\n",
    "    # 2) Now you can just pull from env anywhere\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    # 3) Get your user email & dir\n",
    "    email, user_dir = get_user_email()\n",
    "    print(f\"üìÇ Your directory: {user_dir}\")\n",
    "    print(f\"üìß Your email: {email}\")\n",
    "\n",
    "    # 4) Continue with the rest of your pipeline...\n",
    "    run_cv_pipeline(user_dir)\n",
    "    run_outreach_intake()\n",
    "    run_cv_interactive_flow(user_dir, cv_data, llm)\n",
    "    run_cover_letter_interactive_flow(user_dir, cv_data, llm)\n",
    "    send_email_with_cv(user_dir=user_dir, recipient_email=\"shahmeergull20@gmail.com\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
